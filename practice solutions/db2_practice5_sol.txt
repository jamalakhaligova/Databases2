1. exercise 
-----------
We have a table R, a dense index I1 and a sparse index I2 on the table with the following parameters:
T(R) = 10000, bf(R) = 20, bf(I1) = 100, bf(I2) = 100
Compute the following:
B(R)  = ?  -> B(R) = T(R)/bf(R)    = 500
B(I1) = ?  -> B(I1) = T(I1)/bf(I1) = 100  [ I1 dense, so T(I1) = T(R) ]
B(I2) = ?  -> B(I2) = T(I2)/bf(I2) = 5    [ I2 sparse, so T(I2) = B(R) ]

Conclusion: sparse index needs much less space than dense, but it requires the records be sorted.

2. exercise 
-----------
Compute the values in the previous exercise, if the data blocks can be filled 80% full.
-> bf(r) = 0.8 * 20; bf(I1) = 0.8 * 100, the rest of computation is the same as in the previous exercise

3. exercise 
-----------
T(R) = 1000000, bf(R) = 20, we build a B+ tree index on a key column (A) for which bf(I) = 50. 
Give the following:
B(I) = ?    (help: compute the index blocks level by level, starting at the leaf)

leaf level: 
  B(I)=T(I)/bf(I) [T(I)=T(R) because the index is dense] -> B(I)=1000000/50 = 20000
second level:
  B(I)=T(I)/bf(I) [T(I)=B(I), second level index is sparse, so T(I) at second level = B(I) at leaf level] -> B(I) = 20000/50 = 400
third level:
  B(I) = ... 400/50 = 8
fourth level:
  B(I) = ... 8/50 = 1 block (this is the root)
-> So B(I) = 20000 + 400 + 8 + 1 = 20409

What is the cost (in block reads) of an A = c type search (worst case) if
a) the table is stored unordered and we don't use index;  -> B(R)       =  50000  (we have to read all blocks)
b) the table is stored ordered and we don't use index;    -> log2(B(R)) =~ 16     (binary search)
c) we use the above B+ tree index.                        -> ht(I) + 1  =  5      (traverse the tree and follow the pointer)

Conclusion: a B+ tree is much more efficient than a dense index while the size of it is not much larger.

4. exercise (05_optimization.pptx, page 11.)
-----------
Let's consider a relation R with the following parameters:
T(R)=1000000, block size = 4096 bytes, L(R)=128 bytes, V(R,A)=500, and let W the result
of the following query:
W <-- SELECT c1,c2,c3 FROM R WHERE A=x;
Compute B(R) and B(W) if L(W)=64 bytes.

-> compute bf(W) and T(W), then B(W) = T(W)/bf(W)
bf(R) = (block size)/L(R) = 32, bf(W) = (block size)/L(W) = 64; T(W) = T(R)/V(R,A) = 2000
B(R) = T(R)/bf(R) = 1000000/32 =~ 31250
B(W) = T(W)/bf(W) = 2000/64    =~ 32

5. exercise  (05_optimization.pptx, page 21.)
-----------  
Let's suppose we have memory size 101 blocks (M=101), and a table R, where B(R)=1000000.
What is the cost (in block I/O) of a sort operation?
-> 2*B(R) + 2*B(R)*log[B(R)/M] - B(R)  (base of logarithm is M-1)
-> Cost =~ 2000000 + 4000000 - 1000000

6. exercise  (05_optimization.pptx, a) -> p. 28-29., b) -> p. 32., c) -> p. 21. and 31.)
-----------  
Let's suppose we have memory size 101 blocks (M=101), and two tables R and S with the following parameters:
T(R) = 1000000, bf(R) = 10, T(S) = 60000, bf(S) = 12. 
What is the cost (in block read/write) of an equijoin (WHERE R.A = S.A) operation in case of:
a) block-nested loop algorithm   -> B(S)/(M-1)*B(R) + B(S)                              =~ (5000/100)*100000 + 5000 = 5005000
a) if we change the order tables -> B(R)/(M-1)*B(S) + B(R)                              =~ 5100000 
b) hash-join algorithm           -> 3*[B(S)+B(R)]                                       =~ 315000
c) sort-merge join algorithm     -> (Number of passes)*[2*B(S)+2*B(R)] + B(S)+B(R)      =~ 5*105000 = 525000 
   where "Number of passes" can be estimated with log[B(R)/M] (base of logarithm is M-1)

Conclusion: there is a big difference between the efficiency of join algorithms.


7. exercise  (05_optimization.pptx, p. 30., 05_operation_cost_examples.pptx, p. 19-21.)
-----------  
What is the I/O cost of an index-join of the two tables in the previous exercise, 
if we have an index on R (we keep the index in the memory), R is not clustered and
a) V(R,A) = 1000000     -> B(S) + T(S) * T(R)/V(R,A)   ~ 65000
b) V(R,A) = 10000       -> B(S) + T(S) * T(R)/V(R,A)   ~ 6000500

Conclusion: index usage is efficient only when we need a small portion of the records.
